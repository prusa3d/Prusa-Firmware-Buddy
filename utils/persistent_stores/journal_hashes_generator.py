import re
import hashlib
import argparse
from pathlib import Path
import os

CPP_PROGRAM = """
// Contains generated hashes for persistent store
// Hash is calculated as two MSB bytes of SHA-256 bitwise ANDed with 0x3FFF
// Generated by journal_hashes_generator.py

#include <utility_extensions.hpp>
#include <algorithm>
#include <string_view>
#include <array>

namespace journal {{
struct GeneratedPair {{
    std::string_view name;
    uint16_t hashed;
}};
{array_line}

consteval uint16_t get_generated_hash(std::string_view name) {{
    if (auto res = std::ranges::lower_bound(generated_hashes, GeneratedPair{{ .name = name, .hashed = 0 }}, [&](const GeneratedPair &elem, const GeneratedPair &value) {{
            return elem.name < value.name;
        }});
        res != std::end(generated_hashes) && res->name == name) {{
        return res->hashed;
    }}

    consteval_assert_false("Unknown hash");
    return 0xFFFF;
}}
}} // namespace journal
"""

ARRAY_LINE_NOT_EMPTY = """
inline constexpr GeneratedPair generated_hashes [] {{
{data}
}};
"""
ARRAY_LINE_EMPTY = """
inline constexpr std::array<GeneratedPair, 0> generated_hashes {{}};
"""


def find_duplicates(tuples_list):
    num_count = {}  # Dictionary to store the count of each number
    duplicates = set()  # Set to store tuples with duplicate numbers

    for string, num in tuples_list:
        if num in num_count:
            duplicates.add(num_count[num])
            duplicates.add((string, num))
        else:
            num_count[num] = (string, num)

    return duplicates


def main():
    parser = argparse.ArgumentParser(
        "Parse path to store definition and path to output")
    parser.add_argument("Output",
                        help="Path to where to store the output file",
                        type=Path)
    parser.add_argument(
        "Definition",
        help=
        "Paths to files, separated by ;, to parse for hashes"  # mentioning a hash elsewhere for the first time is almost definitely a bug
    )

    args = parser.parse_args()
    definition_files = str(args.Definition).split(';')

    # Define a regular expression pattern to match the hash strings
    regex = re.compile(r'journal::hash\("([^"]+)"\)')

    # Create a list to store the tuples
    hash_tuples = []

    for dfile in definition_files:
        with open(dfile, 'r') as file:
            content = file.read()

        # Find all matches using the regular expression pattern
        matches = regex.findall(content)

        # Calculate the SHA256 hash and perform bitwise AND with 0x3FFF for each match
        for hash_string in matches:
            hash_sha256 = hashlib.sha256(hash_string.encode()).hexdigest(
            )[:4]  # take only first 4 half-bytes
            hash_sha256_masked = int(hash_sha256, 16) & 0x3FFF
            hash_tuples.append((hash_string, hash_sha256_masked))

        # TODO: Do conflict check only on per file basis, not like how it's done now only at the very end

    hash_tuples.sort(key=lambda x: x[0])

    array_line_str = ""

    if not hash_tuples:
        array_line_str = ARRAY_LINE_EMPTY
    else:
        data_string = ""
        for hash_tuple in hash_tuples:
            data_string += f"        {{ \"{hash_tuple[0]}\", {str(hash_tuple[1])} }},\n"
        # remove last newline so that the array_line string can have it
        data_string = data_string[:-1]
        array_line_str = ARRAY_LINE_NOT_EMPTY.format(data=data_string)

    duplicate_tuples = find_duplicates(hash_tuples)
    if duplicate_tuples:
        error_string = "Persistent store hash ids have at least one hash colision. Conflicts:\n"
        for item in duplicate_tuples:
            error_string += f"('{item[0]}':{item[1]})\n"
        error_string = error_string[:-1]
        raise (Exception(error_string))

    dirname = Path(os.path.dirname(args.Output))
    dirname.mkdir(parents=True, exist_ok=True)
    with open(args.Output, 'w') as file:
        file.write(CPP_PROGRAM.format(array_line=array_line_str))


if __name__ == "__main__":
    main()
